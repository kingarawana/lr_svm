{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = 'data/linear_svm.pkl'\n",
    "with open(data_path, 'r') as f:\n",
    "    datadict = pickle.load(f)\n",
    "    X = datadict['X']\n",
    "    y = datadict['y']\n",
    "\n",
    "W = np.random.randn(10, 3073) * 0.0001 \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check implementation with numeric method\n",
    "\n",
    "from files.linear_svm import *\n",
    "from files.gradient_check import grad_check\n",
    "\n",
    "loss, grad = svm_loss_naive(W, X, y, 0)\n",
    "f = lambda w: svm_loss_naive(w, X, y, 0.0)[0]\n",
    "\n",
    "grad_numerical = grad_check(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare naive to vectorized\n",
    "\n",
    "from files.linear_svm import *\n",
    "start = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(W, X, y, 0.00001)\n",
    "stop = time.time()\n",
    "naive_time = stop - start\n",
    "\n",
    "print 'Naive loss: %e computed in %fs' % (loss_naive, stop - start)\n",
    "\n",
    "start = time.time()\n",
    "loss_vectorized, grad_vectorized = svm_loss_vectorized(W, X, y, 0.00001)\n",
    "stop = time.time()\n",
    "vectorized_time = stop - start\n",
    "\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "\n",
    "print 'Vectorized loss: %e computed in %fs' % (loss_vectorized, stop - start)\n",
    "# The loss difference should be 0, but your vectorized implementation should be much faster.\n",
    "print 'Loss difference: %f. Vectorized version is %fx faster' % (loss_naive - loss_vectorized, naive_time/vectorized_time)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "\n",
    "from files.linear_classifier import LinearSVM\n",
    "svm = LinearSVM()\n",
    "start = time.time()\n",
    "loss_hist = svm.train(X, y, learning_rate=1e-7, reg=5e4, num_iters=1500)\n",
    "stop = time.time()\n",
    "print 'Training time %fs' % (stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
