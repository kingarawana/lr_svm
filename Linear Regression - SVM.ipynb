{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = 'data/linear_svm.pkl'\n",
    "with open(data_path, 'r') as f:\n",
    "    datadict = pickle.load(f)\n",
    "    X = datadict['X']\n",
    "    y = datadict['y']\n",
    "\n",
    "W = np.random.randn(10, 3073) * 0.0001 \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 15.858824 analytic: 15.858824, relative error: 8.548819e-12\n",
      "numerical: -2.185730 analytic: -2.185730, relative error: 1.031397e-10\n",
      "numerical: 28.973165 analytic: 28.973165, relative error: 7.401516e-12\n",
      "numerical: 15.152454 analytic: 14.952734, relative error: 6.634073e-03\n",
      "numerical: -4.134931 analytic: -4.345596, relative error: 2.484109e-02\n",
      "numerical: -14.095309 analytic: -14.095309, relative error: 2.304550e-12\n",
      "numerical: 9.658232 analytic: 9.658232, relative error: 1.026687e-11\n",
      "numerical: 45.960769 analytic: 45.960769, relative error: 3.737167e-12\n",
      "numerical: -4.343019 analytic: -4.343019, relative error: 4.200832e-11\n",
      "numerical: 30.049375 analytic: 30.049375, relative error: 2.344781e-12\n"
     ]
    }
   ],
   "source": [
    "from files.linear_svm import *\n",
    "from files.gradient_check import grad_check\n",
    "\n",
    "loss, grad = svm_loss_naive(W, X, y, 0)\n",
    "f = lambda w: svm_loss_naive(w, X, y, 0.0)[0]\n",
    "\n",
    "grad_numerical = grad_check(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 9.394653e+00 computed in 0.018076s\n",
      "Vectorized loss: 9.394653e+00 computed in 0.001949s\n",
      "difference: 0.000000. Vectorized version is 9.275263x faster\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(W, X, y, 0.00001)\n",
    "stop = time.time()\n",
    "naive_time = stop - start\n",
    "\n",
    "print 'Naive loss: %e computed in %fs' % (loss_naive, stop - start)\n",
    "\n",
    "start = time.time()\n",
    "loss_vectorized, _ = svm_loss_vectorized(W, X, y, 0.00001)\n",
    "stop = time.time()\n",
    "vectorized_time = stop - start\n",
    "\n",
    "print 'Vectorized loss: %e computed in %fs' % (loss_vectorized, stop - start)\n",
    "\n",
    "# The loss difference should be 0, but your vectorized implementation should be much faster.\n",
    "print 'difference: %f. Vectorized version is %fx faster' % (loss_naive - loss_vectorized, naive_time/vectorized_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 26.814590\n",
      "iteration 100 / 1500: loss 5.618111\n",
      "iteration 200 / 1500: loss 2.677176\n",
      "iteration 300 / 1500: loss 1.753109\n",
      "iteration 400 / 1500: loss 1.387087\n",
      "iteration 500 / 1500: loss 1.143192\n",
      "iteration 600 / 1500: loss 1.125558\n",
      "iteration 700 / 1500: loss 1.064710\n",
      "iteration 800 / 1500: loss 1.085315\n",
      "iteration 900 / 1500: loss 1.008493\n",
      "iteration 1000 / 1500: loss 1.011268\n",
      "iteration 1100 / 1500: loss 1.065401\n",
      "iteration 1200 / 1500: loss 1.093950\n",
      "iteration 1300 / 1500: loss 1.010721\n",
      "iteration 1400 / 1500: loss 0.947021\n",
      "Training time 2.362395s\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier\n",
    "\n",
    "from files.linear_classifier import LinearSVM\n",
    "svm = LinearSVM()\n",
    "start = time.time()\n",
    "loss_hist = svm.train(X, y, learning_rate=1e-7, reg=5e4, num_iters=1500)\n",
    "stop = time.time()\n",
    "print 'Training time %fs' % (stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
